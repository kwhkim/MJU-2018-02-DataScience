## 의사 결정 나무의 장점과 단점

* 장점
    - 비선형성, 상호작용을 자동적으로 찾아낸다.
        - 숫자형, 범주형 데이터에 대해 전처리 없이 사용할 수 있다.
    - 자동적으로 변수를 선택한다.
    - 결측치에 대해서도 자동적으로 처리한다.
    - 결과를 해석하기 비교적 쉽다.
* 단점
    - **과적합되기 쉽다.**
    - **분산이 크다.**
         - 동일한 모집단에서 추출한 표본 자료에 대해서 다른 구조를 만들어 낼 가능성이 크다.
    - 다른 알고리즘에 비해 예측 정확성이 낮을 수 있다.

## 과적합, 모형 복잡도, 그리고 모형 분산

* 과적합(overfitting) : 훈련에 사용된 자료에 대해서는 예측이 정확하지만, 새로운 자료에 대해서는 예측이 부정확해지는 현상.
* 모형 분산(model variance) : 동일한 모집단에서 추출된 자료에 대한 적합된 모형의 변동성.
    - 모형 분산은 모집단의 실제 모형, 표본 크기, 그리고 적합하는 모형의 함수이다.
* 모형 복잡도(model complexity) : 모형이 나타낼 수 있는 모형의 다양성.

## 배깅(Bagging)

* 배깅(Bagging) 
    - **부트스트랩 애그리게이션(Bootstrap Aggregation)**의 약자이다.
    - 여러 모형을 동시에 활용하여 예측을 하는 **앙상블(ensemble) 방법**의 하나이다.
    - 주어진 데이터에서 부트스트랩 샘플을 뽑아서 의사결정나무를 만든다.
    - 최종 모형은 여러 의사결정나무의 평균이 된다. 
    - 의사결정나무의 **모형 분산**을 줄이기 위해 사용될 수 있다.
    - 일반적으로 **과적합** 확률 또한 낮아진다.
    
# 예. 스팸 탐지하기

```{r}
# 데이터 설명 : https://archive.ics.uci.edu/ml/datasets/spambase
datSpam <- read.csv('https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data', sep=',', header=FALSE)
head(datSpam)
summary(datSpam)

```

* 몇 가지 특징
    - 스팸 : 광고, "빨리 돈 벌기" 사기, 행운의 편지 등
    - 정상 메일을 스팸으로 오탐지하는 확률을 최대한 낮출 필요가 있다.
    





